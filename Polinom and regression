from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import RidgeCV
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

df = pd.read_csv("/content/drive/MyDrive/Python stat/Ames.csv")
df.head()

df = pd.get_dummies(df, drop_first=True)

X = df.drop("SalePrice", axis=1)
y = df["SalePrice"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

poly_converter = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly_converter.fit_transform(X_train_scaled)
X_test_poly = poly_converter.transform(X_test_scaled)

ridge_cv_model = RidgeCV(alphas=(0.1, 1.0, 10.0), scoring="neg_mean_squared_error", cv=5)
ridge_cv_model.fit(X_train_poly, y_train)

optimal_alpha = ridge_cv_model.alpha_

predictions = ridge_cv_model.predict(X_test_poly)

rmse = np.sqrt(mean_squared_error(y_test, predictions))
r_squared = ridge_cv_model.score(X_test_poly, y_test)
percent_in_mean = (abs(y_test - predictions) / y_test.mean() * 100).mean()
print("Optimal Alpha:", optimal_alpha)
print("RMSE:", rmse)
print("R-squared:", r_squared)
print("% in mean:", percent_in_mean)

coefficients = pd.DataFrame({
    'Variable': poly_converter.get_feature_names_out(X.columns),
    'Coefficient': ridge_cv_model.coef_})

top_features = coefficients.sort_values(by='Coefficient', ascending=False).head(10)
print(top_features)

plt.figure(figsize=(12, 6))
plt.bar(top_features['Variable'], top_features['Coefficient'])
plt.title('Top 10 Important Features')
plt.xlabel('Variable')
plt.ylabel('Coefficient')
plt.xticks(rotation=45, ha='right')
plt.show()
